# Red Neuronal XOR con MÃ©todos de IntegraciÃ³n NumÃ©rica

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyQt5](https://img.shields.io/badge/PyQt5-5.15+-green.svg)](https://pypi.org/project/PyQt5/)
[![NumPy](https://img.shields.io/badge/NumPy-1.21+-orange.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-red.svg)](https://matplotlib.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa una **red neuronal feedforward** para resolver el problema XOR utilizando diferentes **mÃ©todos de integraciÃ³n numÃ©rica** aplicados a la optimizaciÃ³n de pesos. El proyecto combina conceptos de **ecuaciones diferenciales ordinarias (EDO)** con **aprendizaje automÃ¡tico**, ofreciendo una perspectiva Ãºnica sobre cÃ³mo los mÃ©todos numÃ©ricos influyen en la convergencia y estabilidad del entrenamiento.

### ğŸ¯ Objetivo AcadÃ©mico

Demostrar y comparar cÃ³mo diferentes mÃ©todos de integraciÃ³n numÃ©rica afectan el proceso de optimizaciÃ³n en redes neuronales, tratando el entrenamiento como la soluciÃ³n de un sistema de EDO:

```
dW/dt = -Î· * âˆ‡L(W)
```

Donde:
- `W`: Pesos de la red neuronal
- `Î·`: Tasa de aprendizaje (paso de integraciÃ³n)
- `âˆ‡L`: Gradiente de la funciÃ³n de pÃ©rdida

## ğŸ”¬ MÃ©todos de IntegraciÃ³n Implementados

| MÃ©todo | Orden | Evaluaciones/Paso | PrecisiÃ³n | DescripciÃ³n |
|--------|-------|-------------------|-----------|-------------|
| **Euler ExplÃ­cito** | O(h) | 1 | BÃ¡sica | MÃ©todo mÃ¡s simple, rÃ¡pido pero menos preciso |
| **Runge-Kutta 4 (RK4)** | O(hâ´) | 4 | Alta | MÃ©todo clÃ¡sico de alta precisiÃ³n |
| **Runge-Kutta 4 (RK4)** | O(hâ´) | 4 | Alta | MÃ©todo clÃ¡sico de alta precisiÃ³n |

### ğŸ“Š FÃ³rmulas MatemÃ¡ticas

#### Euler ExplÃ­cito
```
W(t+h) = W(t) + h * f(t, W(t))
```



#### Runge-Kutta 4
```
kâ‚ = h * f(t, W)
kâ‚‚ = h * f(t + h/2, W + kâ‚/2)
kâ‚ƒ = h * f(t + h/2, W + kâ‚‚/2)
kâ‚„ = h * f(t + h, W + kâ‚ƒ)
W(t+h) = W(t) + (kâ‚ + 2kâ‚‚ + 2kâ‚ƒ + kâ‚„)/6
```

## ğŸ—ï¸ Arquitectura de la Red Neuronal

```
Entrada (2) â†’ Capa Oculta (3) â†’ Salida (1)
```

- **Entrada**: 2 neuronas (bits para XOR)
- **Capa Oculta**: 3 neuronas con activaciÃ³n sigmoide
- **Salida**: 1 neurona con activaciÃ³n sigmoide
- **FunciÃ³n de PÃ©rdida**: Error CuadrÃ¡tico Medio (MSE)

### ğŸ“ˆ Problema XOR

| Entrada A | Entrada B | Salida Esperada |
|-----------|-----------|-----------------|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

## ğŸš€ CaracterÃ­sticas


### âœ¨ Funcionalidades Principales

- **Modos de Entrenamiento**: Manual (Euler/RK4), Keras (TensorFlow), SciPy ODE
- **SelecciÃ³n de MÃ©todo Manual**: ComboBox para elegir Euler o RK4
- **Entrenamiento Completo**: OptimizaciÃ³n automÃ¡tica con el modo y mÃ©todo seleccionados
- **Modo Paso a Paso (Manual)**: Permite ejecutar un paso de integraciÃ³n y visualizar gradientes y pesos
- **VisualizaciÃ³n DinÃ¡mica**: Curva de pÃ©rdida y visualizaciÃ³n grÃ¡fica de la red y sus pesos en tiempo real (solo modo Manual)
- **Interfaz GrÃ¡fica Intuitiva**: GUI desarrollada en PyQt5 con panel de control, Ã¡rea de resultados y visualizaciÃ³n

### ğŸ¨ Interfaz GrÃ¡fica

- **Panel de Control**: ConfiguraciÃ³n de hiperparÃ¡metros
- **Selector de MÃ©todo**: ComboBox para mÃ©todos de integraciÃ³n
- **Ãrea de Resultados**: InformaciÃ³n detallada del entrenamiento
- **VisualizaciÃ³n**: GrÃ¡ficos de convergencia con matplotlib
- **Controles**: Botones para entrenar, paso a paso, y reset

## ğŸ“¦ InstalaciÃ³n

### Prerrequisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)

### Dependencias

```bash
pip install numpy matplotlib PyQt5
```

O usando el archivo de requerimientos:

```bash
pip install -r requirements.txt
```

### InstalaciÃ³n Manual

```bash
# Clonar el repositorio
git clone https://github.com/NicoAV2311/final_ecuaciones_diferenciales.git

# Navegar al directorio
cd final_ecuaciones_diferenciales

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar la aplicaciÃ³n
python "Tentativo final ecuaciones.py"
```

## ğŸ® Uso

### EjecuciÃ³n BÃ¡sica

```bash
python "Tentativo final ecuaciones.py"
```


### Interfaz de Usuario

1. **Configurar ParÃ¡metros**:
   - Tasa de aprendizaje (Î·): 0.01 - 1.0
   - Ã‰pocas: 1000 - 10000
   - Modo: Manual, Keras, SciPy ODE
   - MÃ©todo (solo Manual): Euler o RK4

2. **Entrenar Red**:
   - Clic en "Entrenar / Ejecutar" para optimizaciÃ³n completa
   - Visualizar curva de pÃ©rdida y, en modo Manual, la red y sus pesos

3. **Modo Paso a Paso (Manual)**:
   - Usar "Un Paso (solo Manual)" para ejecutar un paso de integraciÃ³n
   - Visualizar gradientes y pesos actualizados

4. **Resetear**:
   - Clic en "Resetear" para reiniciar el estado interno y la interfaz

### Ejemplo de Uso ProgramÃ¡tico

```python
from Tentativo_final_ecuaciones import SimpleNN

# Crear red con mÃ©todo RK4 (Manual)
nn = SimpleNN(eta=0.1, epochs=2000, method="RK4")
predictions = nn.train()
print(f"PÃ©rdida final: {nn.loss_history[-1]:.6f}")
print(f"Predicciones: {predictions}")
```

## ğŸ“Š Resultados Esperados


### Convergencia TÃ­pica

- **Euler**: Convergencia lenta, posible inestabilidad con Î· alto
- **RK4**: Convergencia rÃ¡pida y estable, mayor costo computacional

### MÃ©tricas de Rendimiento

```
MÃ©todo | PrecisiÃ³n Final | Ã‰pocas para 99% | Estabilidad
-------|----------------|-----------------|------------
Euler  | 95-98%         | 3000-5000       | Media
RK4    | 99-100%        | 1000-2000       | Muy Alta
```

## ğŸ§ª Ejemplos de Experimentos


### ComparaciÃ³n de MÃ©todos

```python
# ConfiguraciÃ³n experimental
methods = ["Euler", "RK4"]
learning_rates = [0.05, 0.1, 0.2]
epochs = 2000

for method in methods:
   for lr in learning_rates:
      nn = SimpleNN(eta=lr, epochs=epochs, method=method)
      predictions = nn.train()
      # Analizar convergencia...
```

## ğŸ“ Estructura del Proyecto

```
final_ecuaciones_diferenciales/
â”‚
â”œâ”€â”€ Tentativo final ecuaciones.py    # Archivo principal
â”œâ”€â”€ README.md                        # Este archivo
â”œâ”€â”€ requirements.txt                 # Dependencias
â”œâ”€â”€ LICENSE                         # Licencia del proyecto
â”‚
â”œâ”€â”€ docs/                           # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ mathematical_background.md  # Fundamentos matemÃ¡ticos
â”‚   â””â”€â”€ user_guide.md              # GuÃ­a detallada de usuario
â”‚
â””â”€â”€ examples/                       # Ejemplos y experimentos
    â”œâ”€â”€ comparison_analysis.py      # AnÃ¡lisis comparativo
    â””â”€â”€ parameter_tuning.py         # OptimizaciÃ³n de parÃ¡metros
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### ParÃ¡metros de Red

```python
# Arquitectura personalizable
self.W1 = np.random.randn(2, 3)  # Pesos entrada-oculta
self.b1 = np.zeros((1, 3))       # Sesgos capa oculta
self.W2 = np.random.randn(3, 1)  # Pesos oculta-salida
self.b2 = np.zeros((1, 1))       # Sesgos salida
```

### HiperparÃ¡metros Recomendados

| MÃ©todo | Î· (Tasa de Aprendizaje) | Ã‰pocas | Observaciones |
|--------|-------------------------|--------|---------------|
| Euler  | 0.05 - 0.1             | 3000+ | Reducir Î· si hay inestabilidad |
| RK4    | 0.1 - 0.3              | 1500+ | Permite Î· mÃ¡s altos |
| RK4    | 0.1 - 0.3              | 1500+ | Permite Î· mÃ¡s altos |

## ğŸ“– Fundamentos TeÃ³ricos

### Ecuaciones Diferenciales en ML

El entrenamiento de redes neuronales puede modelarse como:

```
dW/dt = -âˆ‡L(W)
```

Este enfoque permite aplicar mÃ©todos de integraciÃ³n numÃ©rica desarrollados para EDO al contexto de optimizaciÃ³n en aprendizaje automÃ¡tico.

### Ventajas del Enfoque EDO

1. **Perspectiva Continua**: Visualizar el entrenamiento como flujo continuo
2. **AnÃ¡lisis de Estabilidad**: Aplicar teorÃ­a de EDO para estudiar convergencia
3. **MÃ©todos Adaptativos**: Potencial para control automÃ¡tico del paso
4. **Interpretabilidad**: ConexiÃ³n clara entre matemÃ¡ticas y ML

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request


### Ãreas de Mejora

- [ ] Implementar mÃ©todos adaptativos (RK45, Dormand-Prince)
- [ ] AÃ±adir soporte para problemas multi-clase
- [ ] Integrar mÃ©tricas avanzadas de anÃ¡lisis numÃ©rico
- [ ] Desarrollar interfaz web con visualizaciones interactivas
- [ ] Implementar comparaciÃ³n automÃ¡tica de mÃ©todos

### Recursos Adicionales

- [DocumentaciÃ³n NumPy](https://numpy.org/doc/)
- [PyQt5 Documentation](https://doc.qt.io/qtforpython/)
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**Nicols Arango Vergara**
- GitHub: [@NicoAV2311](https://github.com/NicoAV2311)
- Universidad: Universidad Catolica Luis Amigo
- Materia: Ecuaciones Diferenciales

## ğŸ“ Contacto

Si tienes preguntas, sugerencias o encuentras algÃºn problema:

- ğŸ› **Issues**: [GitHub Issues](https://github.com/NicoAV2311/final_ecuaciones_diferenciales/issues)
- ğŸ’¬ **Discusiones**: [GitHub Discussions](https://github.com/NicoAV2311/final_ecuaciones_diferenciales/discussions)

---

<div align="center">

**â­ Si este proyecto te fue Ãºtil, por favor dale una estrella en GitHub â­**

*Proyecto desarrollado como trabajo final para la materia de Ecuaciones Diferenciales*

</div>

## ğŸŒ Interfaz Web con Streamlit

A partir de la versiÃ³n 2025, el proyecto incluye una interfaz web interactiva desarrollada con **Streamlit**. Esta versiÃ³n permite ejecutar y visualizar el entrenamiento de la red neuronal XOR directamente en el navegador, con una experiencia moderna y responsiva.

### ğŸš€ InstalaciÃ³n y EjecuciÃ³n de la App Web

#### 1. Instalar Streamlit

```bash
pip install streamlit
```

#### 2. Ejecutar la aplicaciÃ³n web

```bash
streamlit run app_streamlit.py
```

> Si el comando `streamlit` no funciona, prueba:
> ```bash
> python -m streamlit run app_streamlit.py
> ```

#### 3. Acceder desde el navegador

Por defecto, Streamlit abrirÃ¡ la app en [http://localhost:8501](http://localhost:8501)

### ğŸ¨ CaracterÃ­sticas Visuales
- ParÃ¡metros de entrenamiento en panel lateral
- Tabs para curva de pÃ©rdida, resultados finales y visualizaciÃ³n de la red
- Apariencia mejorada con CSS personalizado
- Responsive y fÃ¡cil de usar

### ğŸ“¦ Archivos relevantes
- `app_streamlit.py`: CÃ³digo principal de la app web
- `requirements.txt`: Incluye `streamlit` como dependencia

### ğŸ–¼ï¸ Ejemplo visual

![Streamlit Demo](https://streamlit.io/images/brand/streamlit-mark-color.png)
