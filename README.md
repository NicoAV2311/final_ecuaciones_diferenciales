# Red Neuronal XOR con MÃ©todos de IntegraciÃ³n NumÃ©rica

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyQt5](https://img.shields.io/badge/PyQt5-5.15+-green.svg)](https://pypi.org/project/PyQt5/)
[![NumPy](https://img.shields.io/badge/NumPy-1.21+-orange.svg)](https://numpy.org/)
[![Matplotlib](https://img.shields.io/badge/Matplotlib-3.5+-red.svg)](https://matplotlib.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa una **red neuronal feedforward** para resolver el problema XOR utilizando diferentes **mÃ©todos de integraciÃ³n numÃ©rica** aplicados a la optimizaciÃ³n de pesos. El proyecto combina conceptos de **ecuaciones diferenciales ordinarias (EDO)** con **aprendizaje automÃ¡tico**, ofreciendo una perspectiva Ãºnica sobre cÃ³mo los mÃ©todos numÃ©ricos influyen en la convergencia y estabilidad del entrenamiento.

### ğŸ¯ Objetivo AcadÃ©mico

Demostrar y comparar cÃ³mo diferentes mÃ©todos de integraciÃ³n numÃ©rica afectan el proceso de optimizaciÃ³n en redes neuronales, tratando el entrenamiento como la soluciÃ³n de un sistema de EDO:

```
dW/dt = -Î· * âˆ‡L(W)
```

Donde:
- `W`: Pesos de la red neuronal
- `Î·`: Tasa de aprendizaje (paso de integraciÃ³n)
- `âˆ‡L`: Gradiente de la funciÃ³n de pÃ©rdida

## ğŸ”¬ MÃ©todos de IntegraciÃ³n Implementados

| MÃ©todo | Orden | Evaluaciones/Paso | PrecisiÃ³n | DescripciÃ³n |
|--------|-------|-------------------|-----------|-------------|
| **Euler ExplÃ­cito** | O(h) | 1 | BÃ¡sica | MÃ©todo mÃ¡s simple, rÃ¡pido pero menos preciso |
| **Runge-Kutta 2 (RK2)** | O(hÂ²) | 2 | Media | Balance entre precisiÃ³n y costo computacional |
| **Runge-Kutta 4 (RK4)** | O(hâ´) | 4 | Alta | MÃ©todo clÃ¡sico de alta precisiÃ³n |

### ğŸ“Š FÃ³rmulas MatemÃ¡ticas

#### Euler ExplÃ­cito
```
W(t+h) = W(t) + h * f(t, W(t))
```

#### Runge-Kutta 2 (MÃ©todo del Punto Medio)
```
kâ‚ = h * f(t, W)
kâ‚‚ = h * f(t + h/2, W + kâ‚/2)
W(t+h) = W(t) + kâ‚‚
```

#### Runge-Kutta 4
```
kâ‚ = h * f(t, W)
kâ‚‚ = h * f(t + h/2, W + kâ‚/2)
kâ‚ƒ = h * f(t + h/2, W + kâ‚‚/2)
kâ‚„ = h * f(t + h, W + kâ‚ƒ)
W(t+h) = W(t) + (kâ‚ + 2kâ‚‚ + 2kâ‚ƒ + kâ‚„)/6
```

## ğŸ—ï¸ Arquitectura de la Red Neuronal

```
Entrada (2) â†’ Capa Oculta (3) â†’ Salida (1)
```

- **Entrada**: 2 neuronas (bits para XOR)
- **Capa Oculta**: 3 neuronas con activaciÃ³n sigmoide
- **Salida**: 1 neurona con activaciÃ³n sigmoide
- **FunciÃ³n de PÃ©rdida**: Error CuadrÃ¡tico Medio (MSE)

### ğŸ“ˆ Problema XOR

| Entrada A | Entrada B | Salida Esperada |
|-----------|-----------|-----------------|
| 0 | 0 | 0 |
| 0 | 1 | 1 |
| 1 | 0 | 1 |
| 1 | 1 | 0 |

## ğŸš€ CaracterÃ­sticas

### âœ¨ Funcionalidades Principales

- **SelecciÃ³n de MÃ©todo**: Interfaz para elegir entre Euler, RK2, y RK4
- **Entrenamiento Completo**: OptimizaciÃ³n automÃ¡tica con el mÃ©todo seleccionado
- **Modo Paso a Paso**: AnÃ¡lisis educativo de cada iteraciÃ³n
- **VisualizaciÃ³n en Tiempo Real**: Curvas de pÃ©rdida diferenciadas por color
- **AnÃ¡lisis Comparativo**: MÃ©tricas de convergencia y estabilidad
- **Interfaz Intuitiva**: GUI desarrollada en PyQt5

### ğŸ¨ Interfaz GrÃ¡fica

- **Panel de Control**: ConfiguraciÃ³n de hiperparÃ¡metros
- **Selector de MÃ©todo**: ComboBox para mÃ©todos de integraciÃ³n
- **Ãrea de Resultados**: InformaciÃ³n detallada del entrenamiento
- **VisualizaciÃ³n**: GrÃ¡ficos de convergencia con matplotlib
- **Controles**: Botones para entrenar, paso a paso, y reset

## ğŸ“¦ InstalaciÃ³n

### Prerrequisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)

### Dependencias

```bash
pip install numpy matplotlib PyQt5
```

O usando el archivo de requerimientos:

```bash
pip install -r requirements.txt
```

### InstalaciÃ³n Manual

```bash
# Clonar el repositorio
git clone https://github.com/NicoAV2311/final_ecuaciones_diferenciales.git

# Navegar al directorio
cd final_ecuaciones_diferenciales

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar la aplicaciÃ³n
python "Tentativo final ecuaciones.py"
```

## ğŸ® Uso

### EjecuciÃ³n BÃ¡sica

```bash
python "Tentativo final ecuaciones.py"
```

### Interfaz de Usuario

1. **Configurar ParÃ¡metros**:
   - Tasa de aprendizaje (Î·): 0.01 - 1.0
   - Ã‰pocas: 1000 - 10000
   - MÃ©todo: Euler, RK2, o RK4

2. **Entrenar Red**:
   - Clic en "Entrenar Red" para optimizaciÃ³n completa
   - Observar curva de convergencia en tiempo real

3. **Modo Educativo**:
   - Usar "Un Paso de IntegraciÃ³n" para anÃ¡lisis detallado
   - Visualizar gradientes y actualizaciones paso a paso

4. **AnÃ¡lisis**:
   - Comparar diferentes mÃ©todos
   - Evaluar precisiÃ³n y velocidad de convergencia

### Ejemplo de Uso ProgramÃ¡tico

```python
from Tentativo_final_ecuaciones import SimpleNN

# Crear red con mÃ©todo RK4
nn = SimpleNN(eta=0.1, epochs=2000, method="RK4")

# Entrenar
predictions = nn.train()

# Evaluar resultados
print(f"PÃ©rdida final: {nn.loss_history[-1]:.6f}")
print(f"Predicciones: {predictions}")
```

## ğŸ“Š Resultados Esperados

### Convergencia TÃ­pica

- **Euler**: Convergencia lenta, posible inestabilidad con Î· alto
- **RK2**: Convergencia mÃ¡s suave, mejor estabilidad
- **RK4**: Convergencia rÃ¡pida y estable, mayor costo computacional

### MÃ©tricas de Rendimiento

```
MÃ©todo | PrecisiÃ³n Final | Ã‰pocas para 99% | Estabilidad
-------|----------------|-----------------|------------
Euler  | 95-98%         | 3000-5000      | Media
RK2    | 98-99%         | 2000-3000      | Alta
RK4    | 99-100%        | 1000-2000      | Muy Alta
```

## ğŸ§ª Ejemplos de Experimentos

### ComparaciÃ³n de MÃ©todos

```python
# ConfiguraciÃ³n experimental
methods = ["Euler", "RK2", "RK4"]
learning_rates = [0.05, 0.1, 0.2]
epochs = 2000

# Ejecutar comparaciones y analizar resultados
for method in methods:
    for lr in learning_rates:
        nn = SimpleNN(eta=lr, epochs=epochs, method=method)
        predictions = nn.train()
        # Analizar convergencia...
```

## ğŸ“ Estructura del Proyecto

```
final_ecuaciones_diferenciales/
â”‚
â”œâ”€â”€ Tentativo final ecuaciones.py    # Archivo principal
â”œâ”€â”€ README.md                        # Este archivo
â”œâ”€â”€ requirements.txt                 # Dependencias
â”œâ”€â”€ LICENSE                         # Licencia del proyecto
â”‚
â”œâ”€â”€ docs/                           # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ mathematical_background.md  # Fundamentos matemÃ¡ticos
â”‚   â””â”€â”€ user_guide.md              # GuÃ­a detallada de usuario
â”‚
â””â”€â”€ examples/                       # Ejemplos y experimentos
    â”œâ”€â”€ comparison_analysis.py      # AnÃ¡lisis comparativo
    â””â”€â”€ parameter_tuning.py         # OptimizaciÃ³n de parÃ¡metros
```

## ğŸ”§ ConfiguraciÃ³n Avanzada

### ParÃ¡metros de Red

```python
# Arquitectura personalizable
self.W1 = np.random.randn(2, 3)  # Pesos entrada-oculta
self.b1 = np.zeros((1, 3))       # Sesgos capa oculta
self.W2 = np.random.randn(3, 1)  # Pesos oculta-salida
self.b2 = np.zeros((1, 1))       # Sesgos salida
```

### HiperparÃ¡metros Recomendados

| MÃ©todo | Î· (Tasa de Aprendizaje) | Ã‰pocas | Observaciones |
|--------|-------------------------|--------|---------------|
| Euler  | 0.05 - 0.1             | 3000+ | Reducir Î· si hay inestabilidad |
| RK2    | 0.1 - 0.2              | 2000+ | Balance Ã³ptimo |
| RK4    | 0.1 - 0.3              | 1500+ | Permite Î· mÃ¡s altos |

## ğŸ“– Fundamentos TeÃ³ricos

### Ecuaciones Diferenciales en ML

El entrenamiento de redes neuronales puede modelarse como:

```
dW/dt = -âˆ‡L(W)
```

Este enfoque permite aplicar mÃ©todos de integraciÃ³n numÃ©rica desarrollados para EDO al contexto de optimizaciÃ³n en aprendizaje automÃ¡tico.

### Ventajas del Enfoque EDO

1. **Perspectiva Continua**: Visualizar el entrenamiento como flujo continuo
2. **AnÃ¡lisis de Estabilidad**: Aplicar teorÃ­a de EDO para estudiar convergencia
3. **MÃ©todos Adaptativos**: Potencial para control automÃ¡tico del paso
4. **Interpretabilidad**: ConexiÃ³n clara entre matemÃ¡ticas y ML

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crear una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agregar nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Crear un Pull Request

### Ãreas de Mejora

- [ ] Implementar mÃ©todos adaptativos (RK45, Dormand-Prince)
- [ ] AÃ±adir soporte para problemas multi-clase
- [ ] Integrar mÃ©tricas avanzadas de anÃ¡lisis numÃ©rico
- [ ] Desarrollar interfaz web con visualizaciones interactivas
- [ ] Implementar comparaciÃ³n automÃ¡tica de mÃ©todos

## ğŸ“š Referencias

### Papers y Libros

1. Butcher, J.C. (2016). *Numerical Methods for Ordinary Differential Equations*
2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*
3. Press, W.H., et al. (2007). *Numerical Recipes: The Art of Scientific Computing*

### Recursos Adicionales

- [DocumentaciÃ³n NumPy](https://numpy.org/doc/)
- [PyQt5 Documentation](https://doc.qt.io/qtforpython/)
- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/)

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para mÃ¡s detalles.

## ğŸ‘¨â€ğŸ’» Autor

**NicolÃ¡s AndrÃ©s Villamizar**
- GitHub: [@NicoAV2311](https://github.com/NicoAV2311)
- Universidad: [Tu Universidad]
- Materia: Ecuaciones Diferenciales

## ğŸ“ Contacto

Si tienes preguntas, sugerencias o encuentras algÃºn problema:

- ğŸ› **Issues**: [GitHub Issues](https://github.com/NicoAV2311/final_ecuaciones_diferenciales/issues)
- ğŸ“§ **Email**: [tu-email@universidad.edu]
- ğŸ’¬ **Discusiones**: [GitHub Discussions](https://github.com/NicoAV2311/final_ecuaciones_diferenciales/discussions)

---

<div align="center">

**â­ Si este proyecto te fue Ãºtil, por favor dale una estrella en GitHub â­**

*Proyecto desarrollado como trabajo final para la materia de Ecuaciones Diferenciales*

</div>
